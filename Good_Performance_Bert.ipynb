{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wOxq9LE-i9SK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "D6MgmCd9jFrz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/English_profanity_words.csv\")\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['clean_text'].tolist(),\n",
        "    df['is_offensive'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UCRoYIlYjOjs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "HjEpuQTnjSar"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProfanityDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)  # âœ… Correct dtype\n",
        "        return item"
      ],
      "metadata": {
        "id": "CfjaiiFOjXG8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ProfanityDataset(train_encodings, train_labels)\n",
        "test_dataset = ProfanityDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "VxaotBDSjXvf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=5, hidden_dropout_prob=0.5)\n",
        "\n",
        "# âœ… Load BERT with the correct classifier\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onJFZLv4jZ27",
        "outputId": "6bf6de28-ea79-4cb7-f9f3-e401403c4dc5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "oo804CFMje0L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()  # Logs into your wandb account"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "7eQOAufdmgP0",
        "outputId": "87559eb3-4b0c-44d0-9c2c-d33cd8516b9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspandan-maitra-2004\u001b[0m (\u001b[33mspandan-maitra-2004-sikkim-manipal-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    logging_dir=\"./temp_logs\",  # âœ… Temporary logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[\"wandb\"],  # âœ… Logs to wandb\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "# âœ… Compute Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = torch.argmax(torch.tensor(logits), axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "# âœ… Initialize Trainer with Early Stopping\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "# âœ… Start wandb logging\n",
        "wandb.init(project=\"bert-finetune\", name=\"profanity-filter\")\n",
        "\n",
        "# âœ… Estimate Training Time\n",
        "num_samples = len(train_dataset)\n",
        "num_steps_per_epoch = num_samples // training_args.per_device_train_batch_size\n",
        "total_steps = num_steps_per_epoch * training_args.num_train_epochs\n",
        "gpu_factor = 1 if torch.cuda.is_available() else 4\n",
        "\n",
        "estimated_time = (total_steps * 0.3) / gpu_factor\n",
        "print(f\"ðŸš€ Estimated Training Time: ~{estimated_time:.2f} seconds (~{estimated_time/60:.2f} minutes)\")\n",
        "\n",
        "# âœ… Train Model & Track Actual Time\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "# âœ… Print Actual Training Time\n",
        "actual_time = end_time - start_time\n",
        "print(f\"âœ… Training Complete! Actual Training Time: {actual_time:.2f} seconds (~{actual_time/60:.2f} minutes)\")\n",
        "\n",
        "# âœ… Evaluate Model\n",
        "results = trainer.evaluate()\n",
        "print(f\"âœ… Final Accuracy: {results['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "ColgrpJ6jfoq",
        "outputId": "9fc23b44-0484-4179-b06e-066bee5040cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250322_161844-kyl9xjd5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/spandan-maitra-2004-sikkim-manipal-institute-of-technology/bert-finetune/runs/kyl9xjd5' target=\"_blank\">profanity-filter</a></strong> to <a href='https://wandb.ai/spandan-maitra-2004-sikkim-manipal-institute-of-technology/bert-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/spandan-maitra-2004-sikkim-manipal-institute-of-technology/bert-finetune' target=\"_blank\">https://wandb.ai/spandan-maitra-2004-sikkim-manipal-institute-of-technology/bert-finetune</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/spandan-maitra-2004-sikkim-manipal-institute-of-technology/bert-finetune/runs/kyl9xjd5' target=\"_blank\">https://wandb.ai/spandan-maitra-2004-sikkim-manipal-institute-of-technology/bert-finetune/runs/kyl9xjd5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Estimated Training Time: ~2764.80 seconds (~46.08 minutes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9218' max='9218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9218/9218 32:29, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.145100</td>\n",
              "      <td>0.195163</td>\n",
              "      <td>0.944889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.113500</td>\n",
              "      <td>0.177656</td>\n",
              "      <td>0.946082</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Complete! Actual Training Time: 1955.13 seconds (~32.59 minutes)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1153' max='1153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1153/1153 00:58]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Final Accuracy: 0.9461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_profanity(text):\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    label_map = {0: \"Not Profane\", 1: \"Profane\"}\n",
        "    return label_map[prediction]\n",
        "\n",
        "# âœ… Example Tests\n",
        "test_sentences = [\n",
        "    \"You are amazing!\",\n",
        "    \"This is a terrible f***ing idea!\",\n",
        "    \"Shes a nga\",\n",
        "    \"let's call black people the n word\",\n",
        "    \"Gook\",\n",
        "    \"It seems, your a bit too smart.\",\n",
        "    \"Your brain is smaller than peanuts.\",\n",
        "    \"Women should always stay in the kitchen.\",\n",
        "    \"The only difference between you and midnight is 11:59 pm.\",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Prediction: {predict_profanity(sentence)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnG8ODczxAfv",
        "outputId": "a85032dc-68d1-4063-c156-7c94014a9fad"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: You are amazing!\n",
            "Prediction: Not Profane\n",
            "\n",
            "Input: This is a terrible f***ing idea!\n",
            "Prediction: Profane\n",
            "\n",
            "Input: Shes a nga\n",
            "Prediction: Profane\n",
            "\n",
            "Input: let's call black people the n word\n",
            "Prediction: Profane\n",
            "\n",
            "Input: Gook\n",
            "Prediction: Profane\n",
            "\n",
            "Input: It seems, your a bit too smart.\n",
            "Prediction: Not Profane\n",
            "\n",
            "Input: Your brain is smaller than peanuts.\n",
            "Prediction: Profane\n",
            "\n",
            "Input: Women should always stay in the kitchen.\n",
            "Prediction: Not Profane\n",
            "\n",
            "Input: The only difference between you and midnight is 11:59 pm.\n",
            "Prediction: Not Profane\n",
            "\n",
            "Input: madarchod\n",
            "Prediction: Not Profane\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# âœ… Define save path\n",
        "save_directory = \"/content/bert_profanity_model\"\n",
        "\n",
        "# âœ… Ensure directory exists\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# âœ… Save model & tokenizer\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"âœ… Model & Tokenizer saved at: {save_directory}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWoP-yHhxupH",
        "outputId": "23a92356-be38-4925-d78e-b65969e02897"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model & Tokenizer saved at: /content/bert_profanity_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/bert_profanity_model.zip /content/bert_profanity_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u1L8r2VyEWA",
        "outputId": "c9c2bcd1-d8a7-44b4-8b4e-3920accd6db0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/bert_profanity_model/ (stored 0%)\n",
            "  adding: content/bert_profanity_model/config.json (deflated 53%)\n",
            "  adding: content/bert_profanity_model/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/bert_profanity_model/vocab.txt (deflated 53%)\n",
            "  adding: content/bert_profanity_model/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/bert_profanity_model/model.safetensors (deflated 7%)\n"
          ]
        }
      ]
    }
  ]
}