{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOxq9LE-i9SK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "D6MgmCd9jFrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/English_profanity_words.csv\")\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['clean_text'].tolist(),\n",
        "    df['is_offensive'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UCRoYIlYjOjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
      ],
      "metadata": {
        "id": "HjEpuQTnjSar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProfanityDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item"
      ],
      "metadata": {
        "id": "CfjaiiFOjXG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ProfanityDataset(train_encodings, train_labels)\n",
        "test_dataset = ProfanityDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "VxaotBDSjXvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=5, hidden_dropout_prob=0.5)\n",
        "\n",
        "# Load BERT with the correct classifier\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)"
      ],
      "metadata": {
        "id": "onJFZLv4jZ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=\"Your API token here\")"
      ],
      "metadata": {
        "id": "oo804CFMje0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()  # Logs into your wandb account"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7eQOAufdmgP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=2e-5,\n",
        "    logging_dir=\"./temp_logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[\"wandb\"],\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        ")\n",
        "\n",
        "# Compute Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = torch.argmax(torch.tensor(logits), axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "# Initialize Trainer with Early Stopping\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "# Start wandb logging\n",
        "wandb.init(project=\"bert-finetune\", name=\"profanity-filter\")\n",
        "\n",
        "# Estimate Training Time\n",
        "num_samples = len(train_dataset)\n",
        "num_steps_per_epoch = num_samples // training_args.per_device_train_batch_size\n",
        "total_steps = num_steps_per_epoch * training_args.num_train_epochs\n",
        "gpu_factor = 1 if torch.cuda.is_available() else 4\n",
        "\n",
        "estimated_time = (total_steps * 0.3) / gpu_factor\n",
        "print(f\"ðŸš€ Estimated Training Time: ~{estimated_time:.2f} seconds (~{estimated_time/60:.2f} minutes)\")\n",
        "\n",
        "# Train Model & Track Actual Time\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "# Print Actual Training Time\n",
        "actual_time = end_time - start_time\n",
        "print(f\"âœ… Training Complete! Actual Training Time: {actual_time:.2f} seconds (~{actual_time/60:.2f} minutes)\")\n",
        "\n",
        "# Evaluate Model\n",
        "results = trainer.evaluate()\n",
        "print(f\"âœ… Final Accuracy: {results['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "ColgrpJ6jfoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}